{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db577c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trainer — AI-Powered Digital Epidemiologist (Modular & Professional)\n",
    "\n",
    "# **What this notebook does**\n",
    "# 1. Generate a synthetic vitals dataset (~ configurable size; default 210k rows)\n",
    "# 2. Compute and save dataset statistics (JSON) for poster use\n",
    "# 3. Create and save visualizations (histogram, boxplot, pie, line, scatter)\n",
    "# 4. Preprocess (scaling) and save `scaler.pkl`\n",
    "# 5. Train a classification model (PyTorch if available; else sklearn RandomForest)\n",
    "#    - Training includes logging, checkpointing, optional early stopping and best-model saving\n",
    "# 6. Save final artifacts to `model/artifacts/` and plots to `plots/`\n",
    "\n",
    "# **How to run**\n",
    "# - Copy cells in order and run them.\n",
    "# - Tune hyperparameters in *Cell 2 (Configuration)* before running later cells.\n",
    "# - If you want a full training on the entire dataset, set `TRAIN_ON_FULL_DATA = True` in cell 2 (but expect longer runtime).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2b2fa91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\n",
      "Contents: ['artifacts', 'train.py', 'trainer.ipynb']\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — Debug CWD (run first)\n",
    "from pathlib import Path\n",
    "cwd = Path.cwd()\n",
    "print(\"Current working directory:\", cwd)\n",
    "print(\"Contents:\", [p.name for p in cwd.iterdir()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14b04d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTIFACT_DIR: C:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\\artifacts\n",
      "PLOTS_DIR:    C:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\\plots\n",
      "DATASET_SIZE: 210000\n",
      "TRAIN_ON_FULL_DATA: False | SAMPLE_SIZE: 60000\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — Configuration & robust artifact/plots detection\n",
    "from pathlib import Path\n",
    "\n",
    "cwd = Path.cwd()\n",
    "\n",
    "# Robust detection for artifacts folder:\n",
    "# If running inside model/ and artifacts exists: use cwd/artifacts\n",
    "# Else if repo-root has model/artifacts: use that\n",
    "if (cwd / \"artifacts\").exists():\n",
    "    ARTIFACT_DIR = cwd / \"artifacts\"\n",
    "elif (cwd / \"model\" / \"artifacts\").exists():\n",
    "    ARTIFACT_DIR = cwd / \"model\" / \"artifacts\"\n",
    "else:\n",
    "    # default: create artifacts next to notebook\n",
    "    ARTIFACT_DIR = cwd / \"artifacts\"\n",
    "\n",
    "PLOTS_DIR = cwd / \"plots\"\n",
    "\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dataset & training configuration (tweak these as needed)\n",
    "DATASET_SIZE = 210_000       # >= 200,000 as requested (default 210k)\n",
    "SEED = 42\n",
    "\n",
    "TRAIN_ON_FULL_DATA = False   # If True, trains on whole DATASET_SIZE (can be slow)\n",
    "SAMPLE_SIZE = 60_000         # sample size used for training if not training on full data\n",
    "VAL_SPLIT = 0.15             # validation fraction\n",
    "\n",
    "USE_PYTORCH = True           # try PyTorch first, fallback to sklearn if not available\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "HIDDEN_UNITS = 64\n",
    "\n",
    "EARLY_STOPPING = True\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "CHECKPOINT_ON_IMPROVEMENT = True\n",
    "\n",
    "SAVE_SUMMARY_JSON = True\n",
    "\n",
    "print(\"ARTIFACT_DIR:\", ARTIFACT_DIR.resolve())\n",
    "print(\"PLOTS_DIR:   \", PLOTS_DIR.resolve())\n",
    "print(\"DATASET_SIZE:\", DATASET_SIZE)\n",
    "print(\"TRAIN_ON_FULL_DATA:\", TRAIN_ON_FULL_DATA, \"| SAMPLE_SIZE:\", SAMPLE_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1be2f903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:15:47 - INFO - Device set to: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 — Imports & utilities\n",
    "import os, json, time, math, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# optional: use tqdm if available\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    tqdm = lambda x, **kw: x\n",
    "\n",
    "# Logger\n",
    "import logging\n",
    "logger = logging.getLogger(\"trainer\")\n",
    "if not logger.handlers:\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.INFO)\n",
    "    fmt = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", \"%H:%M:%S\")\n",
    "    ch.setFormatter(fmt)\n",
    "    logger.addHandler(ch)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# reproducibility\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# device detection\n",
    "def get_device():\n",
    "    try:\n",
    "        import torch\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    except Exception:\n",
    "        return \"cpu\"\n",
    "\n",
    "set_seed(SEED)\n",
    "DEVICE = get_device()\n",
    "logger.info(f\"Device set to: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b9be1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:15:52 - INFO - Generating synthetic dataset with 210000 rows (seed=42)\n",
      "15:15:53 - INFO - Saved synthetic CSV to: c:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\\artifacts\\synthetic_epidemic_vitals.csv\n",
      "15:15:53 - INFO - Dataset shape: (210000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HR</th>\n",
       "      <th>RR</th>\n",
       "      <th>SpO2</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78.66</td>\n",
       "      <td>18.97</td>\n",
       "      <td>93.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62.52</td>\n",
       "      <td>20.12</td>\n",
       "      <td>92.95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84.01</td>\n",
       "      <td>14.61</td>\n",
       "      <td>95.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>86.29</td>\n",
       "      <td>15.72</td>\n",
       "      <td>96.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.59</td>\n",
       "      <td>16.31</td>\n",
       "      <td>95.91</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      HR     RR   SpO2  Label\n",
       "0  78.66  18.97  93.50      1\n",
       "1  62.52  20.12  92.95      1\n",
       "2  84.01  14.61  95.31      0\n",
       "3  86.29  15.72  96.16      0\n",
       "4  51.59  16.31  95.91      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4 — Generate synthetic dataset (or load if exists)\n",
    "csv_path = ARTIFACT_DIR / \"synthetic_epidemic_vitals.csv\"\n",
    "\n",
    "if csv_path.exists():\n",
    "    logger.info(\"Found existing CSV - loading: %s\", csv_path)\n",
    "    df = pd.read_csv(csv_path)\n",
    "else:\n",
    "    logger.info(\"Generating synthetic dataset with %d rows (seed=%d)\", DATASET_SIZE, SEED)\n",
    "    rng = np.random.default_rng(SEED)\n",
    "\n",
    "    # core distributions\n",
    "    hr = rng.normal(loc=75, scale=12, size=DATASET_SIZE)\n",
    "    rr = rng.normal(loc=16, scale=3, size=DATASET_SIZE)\n",
    "    spo2 = rng.normal(loc=96.5, scale=2.5, size=DATASET_SIZE)\n",
    "\n",
    "    # clamp to plausible physiological bounds\n",
    "    hr = np.clip(hr, 35, 200)\n",
    "    rr = np.clip(rr, 6, 40)\n",
    "    spo2 = np.clip(spo2, 70, 100)\n",
    "\n",
    "    # deterministic rule-based labels (repeatable)\n",
    "    labels = []\n",
    "    for h, r_, s in zip(hr, rr, spo2):\n",
    "        if (s < 92) or (h > 140) or (r_ > 25):\n",
    "            labels.append(2)   # High Risk\n",
    "        elif (92 <= s < 94) or (120 < h <= 140) or (20 < r_ <= 25):\n",
    "            labels.append(1)   # At Risk\n",
    "        else:\n",
    "            labels.append(0)   # Normal\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"HR\": np.round(hr, 2),\n",
    "        \"RR\": np.round(rr, 2),\n",
    "        \"SpO2\": np.round(spo2, 2),\n",
    "        \"Label\": labels\n",
    "    })\n",
    "\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    logger.info(\"Saved synthetic CSV to: %s\", csv_path)\n",
    "\n",
    "logger.info(\"Dataset shape: %s\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4abbe746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:16:01 - INFO - Saved poster-ready stats to c:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\\artifacts\\poster_dataset_stats.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_rows': 210000,\n",
       " 'dataset_columns': 4,\n",
       " 'features': ['HR', 'RR', 'SpO2', 'Label'],\n",
       " 'stats': {'HR': {'count': 210000,\n",
       "   'mean': 74.99011947619047,\n",
       "   'median': 74.96,\n",
       "   'mode': 72.6,\n",
       "   'min': 35.0,\n",
       "   'max': 135.09,\n",
       "   'range': 100.09,\n",
       "   'std': 12.021493378945301},\n",
       "  'RR': {'count': 210000,\n",
       "   'mean': 16.000955904761906,\n",
       "   'median': 16.0,\n",
       "   'mode': 16.2,\n",
       "   'min': 6.0,\n",
       "   'max': 30.02,\n",
       "   'range': 24.02,\n",
       "   'std': 2.994740888214876},\n",
       "  'SpO2': {'count': 210000,\n",
       "   'mean': 96.40052785714286,\n",
       "   'median': 96.49,\n",
       "   'mode': 100.0,\n",
       "   'min': 85.22,\n",
       "   'max': 100.0,\n",
       "   'range': 14.780000000000001,\n",
       "   'std': 2.3313834606617228}},\n",
       " 'label_counts': {0: 160339, 1: 41788, 2: 7873}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 5 — Compute poster-ready statistics and save JSON\n",
    "def series_stats(s: pd.Series):\n",
    "    s_rounded = s.round(1)\n",
    "    mode_vals = s_rounded.mode().tolist()\n",
    "    mode_val = float(mode_vals[0]) if len(mode_vals) > 0 else float(\"nan\")\n",
    "    return {\n",
    "        \"count\": int(s.count()),\n",
    "        \"mean\": float(s.mean()),\n",
    "        \"median\": float(s.median()),\n",
    "        \"mode\": mode_val,\n",
    "        \"min\": float(s.min()),\n",
    "        \"max\": float(s.max()),\n",
    "        \"range\": float(s.max() - s.min()),\n",
    "        \"std\": float(s.std())\n",
    "    }\n",
    "\n",
    "poster_stats = {\n",
    "    \"dataset_rows\": int(df.shape[0]),\n",
    "    \"dataset_columns\": int(df.shape[1]),\n",
    "    \"features\": [\"HR\", \"RR\", \"SpO2\", \"Label\"],\n",
    "    \"stats\": {\n",
    "        \"HR\": series_stats(df[\"HR\"]),\n",
    "        \"RR\": series_stats(df[\"RR\"]),\n",
    "        \"SpO2\": series_stats(df[\"SpO2\"])\n",
    "    },\n",
    "    \"label_counts\": df[\"Label\"].value_counts().sort_index().to_dict()\n",
    "}\n",
    "\n",
    "poster_json_path = ARTIFACT_DIR / \"poster_dataset_stats.json\"\n",
    "with open(poster_json_path, \"w\") as f:\n",
    "    json.dump(poster_stats, f, indent=2)\n",
    "\n",
    "logger.info(\"Saved poster-ready stats to %s\", poster_json_path)\n",
    "poster_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1edffc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:16:11 - INFO - Saved plots to C:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\\plots\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hr_histogram': 'c:\\\\Users\\\\tanma\\\\OneDrive\\\\Desktop\\\\Machine-Learning\\\\model\\\\plots\\\\hr_histogram.png',\n",
       " 'rr_boxplot': 'c:\\\\Users\\\\tanma\\\\OneDrive\\\\Desktop\\\\Machine-Learning\\\\model\\\\plots\\\\rr_boxplot.png',\n",
       " 'label_pie': 'c:\\\\Users\\\\tanma\\\\OneDrive\\\\Desktop\\\\Machine-Learning\\\\model\\\\plots\\\\label_proportions_pie.png',\n",
       " 'spo2_line': 'c:\\\\Users\\\\tanma\\\\OneDrive\\\\Desktop\\\\Machine-Learning\\\\model\\\\plots\\\\spo2_line.png',\n",
       " 'hr_rr_scatter': 'c:\\\\Users\\\\tanma\\\\OneDrive\\\\Desktop\\\\Machine-Learning\\\\model\\\\plots\\\\hr_vs_rr_scatter.png'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 6 — Generate & save visuals (matplotlib)\n",
    "plot_paths = {}\n",
    "\n",
    "# HR Histogram\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(df[\"HR\"], bins=60)\n",
    "plt.title(\"Heart Rate Distribution\")\n",
    "plt.xlabel(\"HR (bpm)\")\n",
    "plt.ylabel(\"Count\")\n",
    "p = PLOTS_DIR / \"hr_histogram.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(p)\n",
    "plt.close()\n",
    "plot_paths[\"hr_histogram\"] = str(p)\n",
    "\n",
    "# RR Boxplot\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.boxplot(df[\"RR\"], vert=True)\n",
    "plt.title(\"Respiratory Rate (Boxplot)\")\n",
    "plt.ylabel(\"RR (breaths/min)\")\n",
    "p = PLOTS_DIR / \"rr_boxplot.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(p)\n",
    "plt.close()\n",
    "plot_paths[\"rr_boxplot\"] = str(p)\n",
    "\n",
    "# Label Pie Chart\n",
    "plt.figure(figsize=(5,5))\n",
    "counts = df[\"Label\"].value_counts().sort_index()\n",
    "plt.pie(counts, labels=[\"Normal (0)\",\"At Risk (1)\",\"High Risk (2)\"], autopct=\"%1.1f%%\", startangle=90)\n",
    "plt.title(\"Label Proportions\")\n",
    "p = PLOTS_DIR / \"label_proportions_pie.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(p)\n",
    "plt.close()\n",
    "plot_paths[\"label_pie\"] = str(p)\n",
    "\n",
    "# SpO2 line (first 2000 samples to show trend)\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(df[\"SpO2\"].values[:2000])\n",
    "plt.title(\"SpO2 Trend (First 2000 samples)\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"SpO2 (%)\")\n",
    "p = PLOTS_DIR / \"spo2_line.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(p)\n",
    "plt.close()\n",
    "plot_paths[\"spo2_line\"] = str(p)\n",
    "\n",
    "# HR vs RR Scatter (sampled)\n",
    "sample_df = df.sample(n=min(5000, len(df)), random_state=SEED)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(sample_df[\"HR\"], sample_df[\"RR\"], s=6, alpha=0.6)\n",
    "plt.title(\"HR vs RR (sampled)\")\n",
    "plt.xlabel(\"HR (bpm)\")\n",
    "plt.ylabel(\"RR (breaths/min)\")\n",
    "p = PLOTS_DIR / \"hr_vs_rr_scatter.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(p)\n",
    "plt.close()\n",
    "plot_paths[\"hr_rr_scatter\"] = str(p)\n",
    "\n",
    "logger.info(\"Saved plots to %s\", PLOTS_DIR.resolve())\n",
    "plot_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d90a8038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:16:40 - INFO - Saved scaler to c:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\\artifacts\\scaler.pkl\n",
      "15:16:40 - INFO - Training will use subsample: 60000 rows\n",
      "15:16:40 - INFO - Train/Val sizes: 51000 / 9000\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 — Scale features, optionally sample for training, and split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_full = df[[\"HR\",\"RR\",\"SpO2\"]].values\n",
    "y_full = df[\"Label\"].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_full)\n",
    "X_full_scaled = scaler.transform(X_full)\n",
    "\n",
    "scaler_path = ARTIFACT_DIR / \"scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "logger.info(\"Saved scaler to %s\", scaler_path)\n",
    "\n",
    "# Choose subset for training to keep runs fast by default\n",
    "if TRAIN_ON_FULL_DATA:\n",
    "    X_for_train = X_full_scaled\n",
    "    y_for_train = y_full\n",
    "    logger.info(\"Training will use full dataset: %d rows\", X_for_train.shape[0])\n",
    "else:\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    idx = rng.choice(len(X_full_scaled), size=min(SAMPLE_SIZE, len(X_full_scaled)), replace=False)\n",
    "    X_for_train = X_full_scaled[idx]\n",
    "    y_for_train = y_full[idx]\n",
    "    logger.info(\"Training will use subsample: %d rows\", X_for_train.shape[0])\n",
    "\n",
    "# Train/validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_for_train, y_for_train, test_size=VAL_SPLIT, random_state=SEED, stratify=y_for_train if len(np.unique(y_for_train))>1 else None)\n",
    "logger.info(\"Train/Val sizes: %d / %d\", X_train.shape[0], X_val.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "492aaf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — Model builders & sklearn fallback\n",
    "\n",
    "def build_torch_model(input_dim=3, hidden=HIDDEN_UNITS, num_classes=3):\n",
    "    import torch.nn as nn\n",
    "    class SimpleNet(nn.Module):\n",
    "        def __init__(self, input_dim=input_dim, hidden=hidden, num_classes=num_classes):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, max(8, hidden//2)),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(max(8, hidden//2), num_classes)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "    return SimpleNet()\n",
    "\n",
    "def train_sklearn_model(X_train, y_train, X_val, y_val, out_path=ARTIFACT_DIR / \"model_sklearn.joblib\"):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=12, n_jobs=-1, random_state=SEED)\n",
    "    clf.fit(X_train, y_train)\n",
    "    joblib.dump(clf, out_path)\n",
    "    logger.info(\"Saved sklearn model to %s\", out_path)\n",
    "    if X_val is not None:\n",
    "        y_pred = clf.predict(X_val)\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        logger.info(\"Validation accuracy (sklearn): %.4f\", acc)\n",
    "    return str(out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b50e1ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 — PyTorch training loop (returns saved checkpoint path)\n",
    "def train_torch(X_train, y_train, X_val, y_val, device=DEVICE, \n",
    "                epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LEARNING_RATE, \n",
    "                checkpoint_path=ARTIFACT_DIR / \"model.pt\", patience=EARLY_STOPPING_PATIENCE):\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "    num_classes = len(np.unique(np.concatenate([y_train, y_val])))\n",
    "    model = build_torch_model(input_dim=X_train.shape[1], hidden=HIDDEN_UNITS, num_classes=num_classes)\n",
    "    model.to(device)\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "    X_tr = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_tr = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_v = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_v = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "    train_ds = TensorDataset(X_tr, y_tr)\n",
    "    val_ds = TensorDataset(X_v, y_v)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    best_state = None\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n_samples = 0\n",
    "        for xb, yb in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=False):\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_n = xb.size(0)\n",
    "            running_loss += loss.item() * batch_n\n",
    "            n_samples += batch_n\n",
    "        train_loss = running_loss / max(1, n_samples)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_loss_total = 0.0\n",
    "        val_samples = 0\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                val_loss_total += loss.item() * xb.size(0)\n",
    "                val_samples += xb.size(0)\n",
    "                preds = out.argmax(dim=1).cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "        val_loss = val_loss_total / max(1, val_samples)\n",
    "        if len(all_preds) > 0:\n",
    "            all_preds = np.concatenate(all_preds)\n",
    "            val_acc = float((all_preds == y_val[:len(all_preds)]).mean())\n",
    "        else:\n",
    "            val_acc = 0.0\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        logger.info(\"Epoch %d/%d — train_loss: %.4f — val_loss: %.4f — val_acc: %.4f\", epoch, epochs, train_loss, val_loss, val_acc)\n",
    "\n",
    "        # checkpoint & early stopping\n",
    "        if val_loss < best_val_loss - 1e-6:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_state = model.state_dict()\n",
    "            if CHECKPOINT_ON_IMPROVEMENT:\n",
    "                torch.save(best_state, checkpoint_path)\n",
    "                logger.info(\"Checkpoint saved to %s\", checkpoint_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if EARLY_STOPPING and epochs_no_improve >= patience:\n",
    "                logger.info(\"Early stopping triggered (no improvement in %d epochs).\", patience)\n",
    "                break\n",
    "\n",
    "    # Save best model\n",
    "    if best_state is not None:\n",
    "        torch.save(best_state, checkpoint_path)\n",
    "        logger.info(\"Best model saved to %s\", checkpoint_path)\n",
    "    else:\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        logger.info(\"Final model saved to %s (no improvement observed)\", checkpoint_path)\n",
    "\n",
    "    # Save history\n",
    "    hist_path = ARTIFACT_DIR / \"training_history.json\"\n",
    "    with open(hist_path, \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    logger.info(\"Training history saved to %s\", hist_path)\n",
    "\n",
    "    return str(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8a05c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:17:02 - INFO - Attempting to train using PyTorch (device: cpu)\n",
      "15:17:04 - INFO - Epoch 1/10 — train_loss: 0.7697 — val_loss: 0.4929 — val_acc: 0.8059\n",
      "15:17:04 - INFO - Checkpoint saved to c:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\\artifacts\\model.pt\n",
      "15:17:04 - INFO - Epoch 2/10 — train_loss: 0.3728 — val_loss: 0.2738 — val_acc: 0.9048\n",
      "15:17:04 - INFO - Checkpoint saved to c:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\\artifacts\\model.pt\n",
      "15:17:05 - INFO - Epoch 3/10 — train_loss: 0.2163 — val_loss: 0.1693 — val_acc: 0.9637\n",
      "15:17:05 - INFO - Checkpoint saved to c:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\\artifacts\\model.pt\n",
      "15:17:05 - INFO - Epoch 4/10 — train_loss: 0.1455 — val_loss: 0.1217 — val_acc: 0.9743\n",
      "15:17:05 - INFO - Checkpoint saved to c:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\\artifacts\\model.pt\n",
      "15:17:06 - INFO - Epoch 5/10 — train_loss: 0.1097 — val_loss: 0.0946 — val_acc: 0.9830\n",
      "15:17:06 - INFO - Checkpoint saved to c:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\\artifacts\\model.pt\n",
      "15:17:06 - INFO - Epoch 6/10 — train_loss: 0.0882 — val_loss: 0.0778 — val_acc: 0.9862\n",
      "15:17:06 - INFO - Checkpoint saved to c:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\\artifacts\\model.pt\n",
      "15:17:07 - INFO - Epoch 7/10 — train_loss: 0.0740 — val_loss: 0.0664 — val_acc: 0.9882\n",
      "15:17:07 - INFO - Checkpoint saved to c:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\\artifacts\\model.pt\n",
      "15:17:07 - INFO - Epoch 8/10 — train_loss: 0.0638 — val_loss: 0.0583 — val_acc: 0.9901\n",
      "15:17:07 - INFO - Checkpoint saved to c:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\\artifacts\\model.pt\n",
      "15:17:08 - INFO - Epoch 9/10 — train_loss: 0.0564 — val_loss: 0.0517 — val_acc: 0.9920\n",
      "15:17:08 - INFO - Checkpoint saved to c:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\\artifacts\\model.pt\n",
      "15:17:08 - INFO - Epoch 10/10 — train_loss: 0.0510 — val_loss: 0.0476 — val_acc: 0.9920\n",
      "15:17:08 - INFO - Checkpoint saved to c:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\\artifacts\\model.pt\n",
      "15:17:08 - INFO - Best model saved to c:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\\artifacts\\model.pt\n",
      "15:17:08 - INFO - Training history saved to c:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\\artifacts\\training_history.json\n",
      "15:17:08 - INFO - Training finished in 6.7 seconds\n",
      "15:17:08 - INFO - Saved dataset and training summary to c:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\\artifacts\\dataset_training_summary.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\tanma\\\\OneDrive\\\\Desktop\\\\Machine-Learning\\\\model\\\\artifacts\\\\model.pt'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 10 — Orchestration & training run\n",
    "model_artifact_path = None\n",
    "training_start = time.time()\n",
    "\n",
    "if USE_PYTORCH:\n",
    "    try:\n",
    "        import torch\n",
    "        logger.info(\"Attempting to train using PyTorch (device: %s)\", DEVICE)\n",
    "        model_artifact_path = train_torch(X_train, y_train, X_val, y_val, device=DEVICE, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LEARNING_RATE)\n",
    "    except Exception as e:\n",
    "        logger.warning(\"PyTorch training failed: %s\", str(e))\n",
    "        USE_PYTORCH = False\n",
    "\n",
    "if not USE_PYTORCH:\n",
    "    logger.info(\"Training with sklearn fallback (RandomForest). This may be slower for large data but is robust.\")\n",
    "    model_artifact_path = train_sklearn_model(X_train, y_train, X_val, y_val, out_path=ARTIFACT_DIR / \"model_sklearn.joblib\")\n",
    "\n",
    "training_end = time.time()\n",
    "duration_sec = training_end - training_start\n",
    "logger.info(\"Training finished in %.1f seconds\", duration_sec)\n",
    "\n",
    "# Save summary JSON\n",
    "summary = {\n",
    "    \"dataset_csv\": str((csv_path).resolve()),\n",
    "    \"poster_stats_json\": str(poster_json_path.resolve()),\n",
    "    \"plots\": plot_paths,\n",
    "    \"scaler\": str(scaler_path.resolve()),\n",
    "    \"model_artifact\": model_artifact_path,\n",
    "    \"training_seconds\": duration_sec,\n",
    "    \"config\": {\n",
    "        \"DATASET_SIZE\": DATASET_SIZE,\n",
    "        \"TRAIN_ON_FULL_DATA\": TRAIN_ON_FULL_DATA,\n",
    "        \"SAMPLE_SIZE\": SAMPLE_SIZE,\n",
    "        \"EPOCHS\": EPOCHS,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        \"HIDDEN_UNITS\": HIDDEN_UNITS\n",
    "    }\n",
    "}\n",
    "if SAVE_SUMMARY_JSON:\n",
    "    with open(ARTIFACT_DIR / \"dataset_training_summary.json\", \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    logger.info(\"Saved dataset and training summary to %s\", ARTIFACT_DIR / \"dataset_training_summary.json\")\n",
    "\n",
    "model_artifact_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27f13976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts written to: C:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\\artifacts\n",
      "- dataset_training_summary.json\n",
      "- model.pt\n",
      "- poster_dataset_stats.json\n",
      "- scaler.pkl\n",
      "- synthetic_epidemic_vitals.csv\n",
      "- training_history.json\n",
      "\n",
      "Plots written to: C:\\Users\\tanma\\OneDrive\\Desktop\\Machine-Learning\\model\\plots\n",
      "- hr_histogram.png\n",
      "- hr_vs_rr_scatter.png\n",
      "- label_proportions_pie.png\n",
      "- rr_boxplot.png\n",
      "- spo2_line.png\n",
      "\n",
      "Dataset sample (5 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HR</th>\n",
       "      <th>RR</th>\n",
       "      <th>SpO2</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>194949</th>\n",
       "      <td>66.65</td>\n",
       "      <td>14.64</td>\n",
       "      <td>97.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161875</th>\n",
       "      <td>69.09</td>\n",
       "      <td>11.93</td>\n",
       "      <td>94.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61912</th>\n",
       "      <td>65.03</td>\n",
       "      <td>18.72</td>\n",
       "      <td>96.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35966</th>\n",
       "      <td>65.38</td>\n",
       "      <td>15.16</td>\n",
       "      <td>97.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143596</th>\n",
       "      <td>68.17</td>\n",
       "      <td>14.12</td>\n",
       "      <td>97.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           HR     RR   SpO2  Label\n",
       "194949  66.65  14.64  97.75      0\n",
       "161875  69.09  11.93  94.34      0\n",
       "61912   65.03  18.72  96.43      0\n",
       "35966   65.38  15.16  97.16      0\n",
       "143596  68.17  14.12  97.16      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 11 — Verify artifacts & sample\n",
    "print(\"Artifacts written to:\", ARTIFACT_DIR.resolve())\n",
    "for p in sorted(ARTIFACT_DIR.glob(\"*\")):\n",
    "    print(\"-\", p.name)\n",
    "\n",
    "print(\"\\nPlots written to:\", PLOTS_DIR.resolve())\n",
    "for p in sorted(PLOTS_DIR.glob(\"*.png\")):\n",
    "    print(\"-\", p.name)\n",
    "\n",
    "print(\"\\nDataset sample (5 rows):\")\n",
    "display(df.sample(5, random_state=SEED))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16926ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 — Optional retrain helper (call retrain() after modifying Cell 2 config)\n",
    "def retrain():\n",
    "    global df, X_full_scaled, X_train, X_val, y_train, y_val, scaler_path, model_artifact_path, plot_paths, poster_stats, poster_json_path\n",
    "    # reload dataset\n",
    "    if not (ARTIFACT_DIR / \"synthetic_epidemic_vitals.csv\").exists():\n",
    "        raise FileNotFoundError(\"synthetic_epidemic_vitals.csv missing; run Cell 4 to generate dataset first.\")\n",
    "    df = pd.read_csv(ARTIFACT_DIR / \"synthetic_epidememic_vitals.csv\")\n",
    "    # recompute stats/plots\n",
    "    poster_stats = {\n",
    "        \"dataset_rows\": int(df.shape[0]),\n",
    "        \"stats\": {\n",
    "            \"HR\": series_stats(df[\"HR\"]),\n",
    "            \"RR\": series_stats(df[\"RR\"]),\n",
    "            \"SpO2\": series_stats(df[\"SpO2\"])\n",
    "        },\n",
    "        \"label_counts\": df[\"Label\"].value_counts().sort_index().to_dict()\n",
    "    }\n",
    "    with open(poster_json_path, \"w\") as f:\n",
    "        json.dump(poster_stats, f, indent=2)\n",
    "    plot_paths = save_plots(df) if 'save_plots' in globals() else None\n",
    "    # scale and split\n",
    "    X_full = df[[\"HR\",\"RR\",\"SpO2\"]].values\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_full)\n",
    "    X_full_scaled = scaler.transform(X_full)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    if not TRAIN_ON_FULL_DATA:\n",
    "        rng = np.random.default_rng(SEED)\n",
    "        idx = rng.choice(len(X_full_scaled), size=min(SAMPLE_SIZE, len(X_full_scaled)), replace=False)\n",
    "        X_for_train = X_full_scaled[idx]\n",
    "        y_for_train = df[\"Label\"].values[idx]\n",
    "    else:\n",
    "        X_for_train = X_full_scaled\n",
    "        y_for_train = df[\"Label\"].values\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_for_train, y_for_train, test_size=VAL_SPLIT, random_state=SEED)\n",
    "    # train\n",
    "    if USE_PYTORCH:\n",
    "        model_artifact_path = train_torch(X_train, y_train, X_val, y_val)\n",
    "    else:\n",
    "        model_artifact_path = train_sklearn_model(X_train, y_train, X_val, y_val)\n",
    "    logger.info(\"Retrain saved model to %s\", model_artifact_path)\n",
    "\n",
    "# Note: this helper is optional; if you want to retrain with different config, modify Cell 2 and call retrain()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
